<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- MathJax -->
    <script
      type="text/javascript"
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <!-- Bootstrap CSS -->
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.0/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-KyZXEAg3QhqLMpG8r+8fhAXLRk2vvoC2f3B09zVXn8CA5QIVfZOJ3BCsw2P0p/We"
      crossorigin="anonymous"
    />

    <title>Work2 Details</title>
  </head>
  <body>
    <!--NaviBar-->
    <nav
      class="navbar navbar-expand-lg navbar-light bg-light py-2"
      style="background-color: #e3f2fd"
    >
      <div class="container-fluid">
        <a class="navbar-brand" href="../index.html">My Works</a>
        <button
          class="navbar-toggler"
          type="button"
          data-bs-toggle="collapse"
          data-bs-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav">
            <li class="nav-item">
              <a
                class="nav-link active"
                aria-current="page"
                href="../index.html"
                >Home</a
              >
            </li>
          </ul>
        </div>
      </div>
    </nav>
    <!--End of NaviBar-->
    <div class="container-fluid">
      <div class="row">
        <hr />
        <div>
          <p class="h5">Details of Work2</p>
          <!--Work2-->
          <div>
            <!--Overview-->
            <div>
              <p class="h6">-Overview</p>
              <p>
                This is an educational app to understand how AI algorithms work.
                In this app, you can see the difference between Policy descent,
                Q-learning, and Sarsa. Policy gradient takes time and learning
                times the most, whereas Q-learning solves the maze quickest and
                looks most efficient. As for Sarsa, it takes the longest time at
                the very beginning of learning and then gets faster.
              </p>
            </div>
            <!--End of Overview-->
            <!--How to play-->
            <div>
              <p class="h6">-How to play</p>
              <p>
                <img src="../img/work2_explanation.png" class="img-thumbnail" />
              </p>
              <ol>
                <li>
                  Select the number of rows and columns. Most outer aria will be
                  walls.
                </li>
                <li>Click to generate the maze</li>
                <li>Select an Algorithm</li>
                <li>Decide how many times it takes to learn</li>
                <li>Click to run</li>
                <li>
                  See the result. If there is no redundant path, which means
                  success. In failed case, there are redundant paths and it gets
                  stronger with the color green as it passes the point.
                </li>
              </ol>
            </div>
            <!--End of How to play-->
            <!--Algorithm-->
            <div>
              <p class="h6">-Algorithm</p>
              <p>Policy Gradient</p>
              <!--Policy Gradient 更新式-->
              <math>
                <mi>&theta;</mi>
                <mfenced>
                  <mi>s</mi>
                  <mi>a</mi>
                </mfenced>
                <mo>←</mo>
                <mi>&theta;</mi>
                <mfenced>
                  <mi>s</mi>
                  <mi>a</mi>
                </mfenced>
                <mo>+</mo>
                <mi>&eta;</mi>
                <mo>*</mo>
                <mfrac>
                  <mrow>
                    <mi>N</mi>
                    <mfenced>
                      <mi>s</mi>
                      <mi>a</mi>
                    </mfenced>
                    <mo>-</mo>
                    <mi>P</mi>
                    <mfenced>
                      <mi>s</mi>
                      <mi>a</mi>
                    </mfenced>
                    <mo>*</mo>
                    <mi>N</mi>
                    <mfenced>
                      <mi>s</mi>
                    </mfenced>
                  </mrow>
                  <mn>T</mn>
                </mfrac>
              </math>
              <!--End of Policy Gradient 更新式-->
              <!--Notations-->
              <div>
                <p>
                  θ: value for the action in the state, s:state, a:action,
                  η:learning rate, N:the number of the action in the state,
                  P:probability of the action in the state, N(s): the number of
                  all actions in the state
                </p>
              </div>
              <!--End of Notations-->
              <p>Q-learning</p>
              <!--Q-learning 更新式-->
              <math>
                <mi>&theta;</mi>
                <mfenced>
                  <msub>
                    <mi>s</mi>
                    <mi>t</mi>
                  </msub>
                  <msub>
                    <mi>a</mi>
                    <mi>t</mi>
                  </msub>
                </mfenced>
                <mo>←</mo>
                <mfenced>
                  <msub>
                    <mi>s</mi>
                    <mi>t</mi>
                  </msub>
                  <msub>
                    <mi>a</mi>
                    <mi>t</mi>
                  </msub>
                </mfenced>
                <mo>+</mo>
                <mi>&eta;</mi>
                <mo>*</mo>
                <mfenced>
                  <mrow>
                    <msub>
                      <mi>R</mi>
                      <mi>t+1</mi>
                    </msub>
                    <mo>+</mo>
                    <mi>&gamma;</mi>
                    <mo>*</mo>
                    <munder>
                      <mi>max</mi>
                      <mrow>
                        <mi>a</mi>
                      </mrow>
                    </munder>
                    <mi>Q</mi>
                    <mfenced>
                      <msub>
                        <mi>s</mi>
                        <mi>t+1</mi>
                      </msub>
                      <msub>
                        <mi>a</mi>
                        <mi>t</mi>
                      </msub>
                    </mfenced>
                    <mo>-</mo>
                    <mi>Q</mi>
                    <mfenced>
                      <msub>
                        <mi>s</mi>
                        <mi>t</mi>
                      </msub>
                      <msub>
                        <mi>a</mi>
                        <mi>t</mi>
                      </msub>
                    </mfenced>
                  </mrow>
                </mfenced>
              </math>
              <!--End of Q-learning 更新式-->
              <div>
                <p>
                  Q:current value, s:state, a:action, η:learning rate, R:reward,
                  γ:discount factor, maxQ: optimal value in the next state
                </p>
              </div>
              <p>Sarsa</p>
              <!--Sarsa 更新式-->
              <math>
                <mi>&theta;</mi>
                <mfenced>
                  <msub>
                    <mi>s</mi>
                    <mi>t</mi>
                  </msub>
                  <msub>
                    <mi>a</mi>
                    <mi>t</mi>
                  </msub>
                </mfenced>
                <mo>←</mo>
                <mfenced>
                  <msub>
                    <mi>s</mi>
                    <mi>t</mi>
                  </msub>
                  <msub>
                    <mi>a</mi>
                    <mi>t</mi>
                  </msub>
                </mfenced>
                <mo>+</mo>
                <mi>&eta;</mi>
                <mo>*</mo>
                <mfenced>
                  <mrow>
                    <msub>
                      <mi>R</mi>
                      <mi>t+1</mi>
                    </msub>
                    <mo>+</mo>
                    <mi>&gamma;</mi>
                    <mo>*</mo>
                    <mi>Q</mi>
                    <mfenced>
                      <msub>
                        <mi>s</mi>
                        <mi>t+1</mi>
                      </msub>
                      <msub>
                        <mi>a</mi>
                        <mi>t+1</mi>
                      </msub>
                    </mfenced>
                    <mo>-</mo>
                    <mi>Q</mi>
                    <mfenced>
                      <msub>
                        <mi>s</mi>
                        <mi>t</mi>
                      </msub>
                      <msub>
                        <mi>a</mi>
                        <mi>t</mi>
                      </msub>
                    </mfenced>
                  </mrow>
                </mfenced>
              </math>
              <!--End of Sarsa 更新式-->
            </div>
            <p></p>
            <!--End of Algorithm-->
            <!--Insight-->
            <div>
              <p class="h6">-Insight</p>
              <p>
                According to the algorithms mentioned above, Policy gradient
                updates its weight for each learning, not each step as the rest
                of the two do, so that it takes a much greater number of
                learning. Q-learning and Sarsa are almost the same, except the
                former is off-policy while the latter is on-policy. Q-learning
                determines Q value, referring the next state and takes the
                maximum value in the state no matter what the next actual action
                is. Note that it sometimes takes a random action in case
                ε-greedy applied. On the other hand, Sarsa determines Q value
                based on the actual next action, so the timing of updating Q
                value is in the next state, which is one step further compared
                with Q-learning.In other words, Q-learning is confident and
                Sarsa is careful as the Cliff Walking(Ref.2) shows.
              </p>
            </div>
            <!--End of Insight-->
            <!--References-->
            <div>
              <p class="h6">-References</p>
              <li>
                1.
                <a
                  href="https://www.amazon.co.jp/gp/product/4862464505/"
                  target="_blank"
                  >AlphaZero 深層学習・強化学習・探索
                  人工知能プログラミング実践入門</a
                >
              </li>
              <li>
                2.
                <a
                  href="https://towardsdatascience.com/reinforcement-learning-cliff-walking-implementation-e40ce98418d4"
                  target="_blank"
                  >Reinforcement Learning — Cliff Walking Implementation</a
                >
              </li>
            </div>

            <!--End of References-->
          </div>
          <!--End of Description-->
        </div>
      </div>
    </div>
    <!-- Option 1: Bootstrap Bundle with Popper -->
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.0/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-U1DAWAznBHeqEIlVSCgzq+c9gqGAJn5c/t99JyeKa9xxaYpSvHU5awsuZVVFIhvj"
      crossorigin="anonymous"
    ></script>
    <!-- Copyright -->
    <div class="text-center bg-light py-2 p-3">
      © 2021 Copyright:
      <a href="https://scikiy.github.io/">Yoshinori SAIKI</a>
    </div>
    <!-- Copyright -->
  </body>
</html>
